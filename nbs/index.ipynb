{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from spear_tts_pytorch.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPEAR-TTS\n",
    "\n",
    "An unofficial PyTorch implementation of [SPEAR-TTS](https://google-research.github.io/seanet/speartts/examples/).\n",
    "\n",
    "We are not targeting an exact copy – to speed up training we want to use existing Open Source models as bases:\n",
    "[Whisper](https://github.com/openai/whisper) encoder to generate semantic tokens and [EnCodec](https://github.com/facebookresearch/encodec) for acoustic modeling.\n",
    "\n",
    "Following Google Brain we'll train on the LibreLight and LibreTTS datasets. Ultimately\n",
    "we want to target multiple languages (Whisper and EnCodec are both multilanguage).\n",
    "\n",
    "UPDATE 2023-02-24: I think I finally figured out how to train the semantic encodings bottleneck. Check the issues for more detailed progress updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper for modeling semantic tokens\n",
    "\n",
    "![Using Whisper for semantic token extraction diagram](whisper-block.png)\n",
    "\n",
    "Pros:\n",
    "\n",
    " - Whisper training should be a lot better at extracting semantic information than a masked language model with\n",
    "   contrastive loss (w2v-VERT)\n",
    " - it's pretrained on 600k hours of multilingual speech (vs. 60k for w2v-BERT used in the paper)\n",
    " - freely available\n",
    "\n",
    "Cons:\n",
    "\n",
    " - 2x higher \"symbol rate\" (50 vec/s) than w2v-BERT (25 vec/s) which means training the semantic->acoustic transformer\n",
    "   may take longer\n",
    " - it seems that we'll need 6x higher symbol rate if we want to quantize the embeddings effectively, OTOH maybe later modeling tasks will be easier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EnCodec for modeling acoustic tokens\n",
    "\n",
    "![EnCodec block diagram](https://github.com/facebookresearch/encodec/raw/main/architecture.png)\n",
    "\n",
    "Pros:\n",
    "\n",
    " - High-quality pretrained model\n",
    "\n",
    "Cons:\n",
    "\n",
    " - Comparing the speech samples with SPEAR-TTS, EnCodec needs 6kbps to get the same quality\n",
    "   (SoundStream retrained only on speech seems to work with 1.5kbps)\n",
    " - CC-BY-NC license\n",
    "\n",
    "We may switch to the [OpenSource SoundStream re-implementation](https://github.com/lucidrains/audiolm-pytorch/blob/main/audiolm_pytorch/soundstream.py) or train a new speech-only model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```bibtex\n",
    "@article{SpearTTS,\n",
    "  title = {Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision},\n",
    "  url = {https://arxiv.org/abs/2302.03540},\n",
    "  author = {Kharitonov, Eugene and Vincent, Damien and Borsos, Zalán and Marinier, Raphaël and Girgin, Sertan and Pietquin, Olivier and Sharifi, Matt and Tagliasacchi, Marco and Zeghidour, Neil},\n",
    "  publisher = {arXiv},\n",
    "  year = {2023},\n",
    "}\n",
    "```\n",
    "\n",
    "```bibtex\n",
    "@article{Whisper\n",
    "  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n",
    "  url = {https://arxiv.org/abs/2212.04356},\n",
    "  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n",
    "  publisher = {arXiv},\n",
    "  year = {2022},\n",
    "}\n",
    "```\n",
    "\n",
    "```bibtex\n",
    "@article{EnCodec\n",
    "  title = {High Fidelity Neural Audio Compression},\n",
    "  url = {https://arxiv.org/abs/2210.13438},\n",
    "  author = {Défossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},\n",
    "  publisher = {arXiv},\n",
    "  year = {2022},\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
